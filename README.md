# Single-Agent Reinforcement Learning for Personalized Federated Learning (pFL)

This repository implements a **single-agent reinforcement learning (RL) framework** for **personalized federated learning (pFL)** under **data and system heterogeneity**.

The core idea is to use **one centralized RL agent at the server** to dynamically control, **for each participating client**:

1. **Which model layers are shared vs personalized**
2. **How much local training to perform (number of local epochs)**

This allows the system to adapt to heterogeneous clients with different data distributions, computational capabilities, and bandwidth constraints, outperforming static personalization strategies.

---

## ğŸ” Motivation

Traditional federated learning methods (e.g., FedAvg) assume:
- homogeneous clients,
- fixed local training schedules,
- static personalization strategies.

In practice, clients vary significantly in:
- data distribution (non-IID),
- compute speed,
- communication bandwidth.

This project explores whether **a single learning agent** can make **adaptive, client-specific decisions** that improve:
- global generalization,
- fairness across clients,
- computational efficiency.

---

## ğŸ§  Key Idea

- There is **only one RL agent** (centralized at the server).
- The agent observes **client states** and outputs **per-client actions**.
- The same policy network is shared across all clients.

```
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Single RL Agent (Server) â”‚
               â”‚   Ï€Î¸(s) â†’ a                â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                    â–¼                    â–¼
     Client 1             Client 2             Client K
   (state sâ‚)           (state sâ‚‚)           (state sâ‚–)
  action aâ‚             action aâ‚‚             action aâ‚–
(layer choice,       (layer choice,       (layer choice,
 local epochs)        local epochs)        local epochs)
```

---

## ğŸ“‚ Repository Structure

```
single_agent_RL_for_pFL/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ resnet18.py              # CIFAR-style ResNet
â”‚   â”œâ”€â”€ RL_model.py              # RL policy network
â”‚
â”œâ”€â”€ data_loaders/
â”‚   â”œâ”€â”€ cifar_10_dataloader.py   # Dirichlet non-IID splits
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ aggregation.py           # FedAvg-style aggregation
â”‚   â”œâ”€â”€ evaluation.py            # Global & client evaluation
â”‚
â”œâ”€â”€ train_rl_federated.py         # Main FL + RL training loop
â”œâ”€â”€ baseline_federated.py         # Non-RL baselines
â”œâ”€â”€ config.py                     # Hyperparameters
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset

- **CIFAR-10**
  - 50,000 training images â†’ split among clients (non-IID)
  - 10,000 test images â†’ **used only for global evaluation**

> The CIFAR-10 **test set is never used for training**.

---

## ğŸ”¬ Non-IID Data Partitioning

- Training data is split across clients using a **Dirichlet distribution**.
- Smaller concentration parameter `Î±` â†’ higher heterogeneity.
- Each client receives a different label distribution and dataset size.

---

## âš™ï¸ Client Heterogeneity

Each client is characterized by:
- local dataset size,
- compute capability,
- bandwidth level,
- participation frequency,
- previous model performance.

These attributes are encoded into the **RL state**.

---

## ğŸ¯ RL Formulation

### State (per client)

A compact vector encoding:
- normalized compute capacity,
- normalized bandwidth,
- local dataset size,
- previous validation accuracy,
- training progress.

### Action (per client)

The RL agent outputs:
- **Layer personalization decision**
  - which blocks are personalized vs shared
  - including support for *middle-layer personalization*
- **Training intensity**
  - number of local epochs

### Reward

A scalar reward computed at the server after each communication round:

```
reward = Î± Â· Î”(global test accuracy)
       âˆ’ Î² Â· variance(client accuracies)
       âˆ’ Î³ Â· compute / communication cost
```

This encourages:
- good global generalization,
- fairness across clients,
- efficient use of resources.

---

## ğŸ“‰ Loss Functions

### Client-Side Learning

Each client trains its local model using standard supervised learning:

\[
\mathcal{L}_{client} = \text{CrossEntropy}(y, f(x))
\]

### RL Agent Loss (Policy Gradient)

The RL agent is trained using a REINFORCE-style objective:

\[
\mathcal{L}_{RL} = - \sum_{t,k}
\log \pi_\theta(a_{k,t} \mid s_{k,t}) \cdot G_t
\]

where \( G_t \) is the discounted return.

The RL loss is **completely separate** from the client training loss.


## ğŸš€ How to Run

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Run RL-based personalized FL
```bash
python train_rl_federated.py
```

### 3. Run non-RL baselines
```bash
python baseline_federated.py
```

---

## ğŸ“ˆ Evaluation Metrics

- **Global accuracy** (CIFAR-10 test set)
- **Client fairness**
  - mean and variance of client accuracies
- **Efficiency**
  - number of local updates
  - communication cost
- **Policy behavior**
  - distribution of selected layers and local epochs

---

## ğŸ“„ Project Report (LaTeX / PDF)

A full technical report (written in LaTeX) describing:
- formal problem formulation,
- RL stateâ€“actionâ€“reward definitions,
- algorithm details,
- experimental results and ablations,

can be included in a `docs/` directory as a compiled PDF.

---

## ğŸ‘¤ Author

**Praditha Alwis**  
PhD Student, Electrical & Computer Engineering  
Purdue University

**Kavindu Herath**  
PhD Student, Electrical & Computer Engineering  
Purdue University

**Nethmi Hewa Withthige**  
PhD Student, Electrical & Computer Engineering  
Purdue University

**Lakshika Karunaratne**  
PhD Student, Electrical & Computer Engineering  
Purdue University

---

## ğŸ“œ License

MIT License
