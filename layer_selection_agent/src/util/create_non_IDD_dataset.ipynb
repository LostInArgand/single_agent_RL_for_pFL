{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc1f3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aabb5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MNIST_dirichlet_clients(\n",
    "    data_root=\"./data\",\n",
    "    num_clients=10,\n",
    "    alpha=0.5,\n",
    "    batch_size=64,\n",
    "    seed=42,\n",
    "):\n",
    "    # 1) CIFAR-10 transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.4914, 0.4822, 0.4465),\n",
    "            std=(0.2023, 0.1994, 0.2010),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # 2) Load CIFAR-10 train set\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "        root=data_root,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train,\n",
    "    )\n",
    "\n",
    "    # 3) Get Dirichlet split indices per client\n",
    "    client_idcs = dirichlet_split_noniid(\n",
    "        dataset=trainset,\n",
    "        num_clients=num_clients,\n",
    "        alpha=alpha,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # 4) Wrap each client’s indices as a Subset + DataLoader\n",
    "    client_loaders = {}\n",
    "    for cid, idxs in client_idcs.items():\n",
    "        subset = Subset(trainset, idxs)\n",
    "        loader = DataLoader(\n",
    "            subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        client_loaders[cid] = loader\n",
    "\n",
    "    return trainset, client_idcs, client_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958df702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Dirichlet non-IID splitter\n",
    "# ==============================\n",
    "def dirichlet_split_noniid(dataset, num_clients, alpha, seed=42):\n",
    "    \"\"\"\n",
    "    Dirichlet label-distribution-based non-IID split.\n",
    "\n",
    "    New semantics:\n",
    "      - alpha = 0   -> IID split (random, equal-size).\n",
    "      - alpha > 0   -> non-IID; larger alpha => more skewed (more non-IID).\n",
    "\n",
    "    Returns:\n",
    "        dict: {client_id: np.ndarray of indices}\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_samples = len(dataset)\n",
    "\n",
    "    # ----- Special case: alpha = 0 => IID -----\n",
    "    if alpha == 0:\n",
    "        all_indices = np.random.permutation(n_samples)\n",
    "        splits = np.array_split(all_indices, num_clients)\n",
    "        client_indices = {\n",
    "            cid: splits[cid] for cid in range(num_clients)\n",
    "        }\n",
    "        return client_indices\n",
    "\n",
    "    # ----- alpha > 0 => Dirichlet-based non-IID -----\n",
    "    # Map user alpha to Dirichlet concentration:\n",
    "    # larger user alpha -> smaller conc -> more non-IID\n",
    "    dirichlet_conc = 1.0 / alpha\n",
    "    # (optional: clip to avoid extreme numerical issues)\n",
    "    dirichlet_conc = float(np.clip(dirichlet_conc, 1e-3, 1e3))\n",
    "\n",
    "    # Get labels generically\n",
    "    if hasattr(dataset, \"targets\"):\n",
    "        labels = np.array(dataset.targets)\n",
    "    elif hasattr(dataset, \"labels\"):\n",
    "        labels = np.array(dataset.labels)\n",
    "    else:\n",
    "        labels = np.array([dataset[i][1] for i in range(n_samples)])\n",
    "\n",
    "    num_classes = int(labels.max()) + 1\n",
    "    client_indices = defaultdict(list)\n",
    "\n",
    "    # For each class, sample Dirichlet over clients\n",
    "    for c in range(num_classes):\n",
    "        class_idx = np.where(labels == c)[0]\n",
    "        if len(class_idx) == 0:\n",
    "            continue\n",
    "        np.random.shuffle(class_idx)\n",
    "\n",
    "        # Dirichlet proportions for this class across clients\n",
    "        proportions = np.random.dirichlet(\n",
    "            dirichlet_conc * np.ones(num_clients)\n",
    "        )\n",
    "\n",
    "        # Convert proportions to index splits\n",
    "        split_points = (np.cumsum(proportions) * len(class_idx)).astype(int)\n",
    "        class_split = np.split(class_idx, split_points[:-1])\n",
    "\n",
    "        for client_id, idx in enumerate(class_split):\n",
    "            client_indices[client_id].extend(idx.tolist())\n",
    "\n",
    "    # Shuffle indices inside each client\n",
    "    for client_id in range(num_clients):\n",
    "        idx = np.array(client_indices[client_id], dtype=int)\n",
    "        np.random.shuffle(idx)\n",
    "        client_indices[client_id] = idx\n",
    "\n",
    "    return client_indices\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# CIFAR-10 + per-client loaders\n",
    "# ==============================\n",
    "def get_cifar10_dirichlet_clients(\n",
    "    data_root=\"./data\",\n",
    "    num_clients=10,\n",
    "    alpha=0.5,\n",
    "    batch_size=64,\n",
    "    seed=42,\n",
    "):\n",
    "    # 1) CIFAR-10 transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.4914, 0.4822, 0.4465),\n",
    "            std=(0.2023, 0.1994, 0.2010),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # 2) Load CIFAR-10 train set\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_root,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train,\n",
    "    )\n",
    "\n",
    "    # 3) Get Dirichlet split indices per client\n",
    "    client_idcs = dirichlet_split_noniid(\n",
    "        dataset=trainset,\n",
    "        num_clients=num_clients,\n",
    "        alpha=alpha,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # 4) Wrap each client’s indices as a Subset + DataLoader\n",
    "    client_loaders = {}\n",
    "    for cid, idxs in client_idcs.items():\n",
    "        subset = Subset(trainset, idxs)\n",
    "        loader = DataLoader(\n",
    "            subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        client_loaders[cid] = loader\n",
    "\n",
    "    return trainset, client_idcs, client_loaders\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# CIFAR-100 + per-client loaders\n",
    "# ==============================\n",
    "def get_cifar100_dirichlet_clients(\n",
    "    data_root=\"./data\",\n",
    "    num_clients=10,\n",
    "    alpha=0.5,\n",
    "    batch_size=64,\n",
    "    seed=42,\n",
    "):\n",
    "    # 1) CIFAR-100 transforms (same as CIFAR-10 usually)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5071, 0.4867, 0.4408),\n",
    "            std=(0.2675, 0.2565, 0.2761),\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # 2) Load CIFAR-100 train set\n",
    "    trainset = torchvision.datasets.CIFAR100(\n",
    "        root=data_root,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform_train,\n",
    "    )\n",
    "\n",
    "    # 3) Get Dirichlet split indices per client\n",
    "    client_idcs = dirichlet_split_noniid(\n",
    "        dataset=trainset,\n",
    "        num_clients=num_clients,\n",
    "        alpha=alpha,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # 4) Wrap each client’s indices as a Subset + DataLoader\n",
    "    client_loaders = {}\n",
    "    for cid, idxs in client_idcs.items():\n",
    "        subset = Subset(trainset, idxs)\n",
    "        loader = DataLoader(\n",
    "            subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        client_loaders[cid] = loader\n",
    "\n",
    "    return trainset, client_idcs, client_loaders\n",
    "\n",
    "# =====================================\n",
    "# Shakespeare + per-client loaders\n",
    "# =====================================\n",
    "def get_shakespeare_dirichlet_clients(\n",
    "    dataset,\n",
    "    num_clients=10,\n",
    "    alpha=0.5,\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    dataset: a PyTorch Dataset for Shakespeare samples.\n",
    "             Must expose labels via .targets/.labels or via dataset[i] -> (x, y).\n",
    "    \"\"\"\n",
    "    # 1) Get Dirichlet split indices per client\n",
    "    client_idcs = dirichlet_split_noniid(\n",
    "        dataset=dataset,\n",
    "        num_clients=num_clients,\n",
    "        alpha=alpha,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # 2) Wrap each client’s indices as a Subset + DataLoader\n",
    "    client_loaders = {}\n",
    "    for cid, idxs in client_idcs.items():\n",
    "        subset = Subset(dataset, idxs)\n",
    "        loader = DataLoader(\n",
    "            subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        client_loaders[cid] = loader\n",
    "\n",
    "    return dataset, client_idcs, client_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f4a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: 13333 samples\n",
      "Client 1: 6581 samples\n",
      "Client 2: 8633 samples\n",
      "Client 3: 2980 samples\n",
      "Client 4: 852 samples\n",
      "Client 5: 10778 samples\n",
      "Client 6: 9289 samples\n",
      "Client 7: 687 samples\n",
      "Client 8: 3635 samples\n",
      "Client 9: 3232 samples\n",
      "Client 0 label histogram:\n",
      "[1401 3556 2482    9  111    1   39 4137  304 1293]\n",
      "Client 1 label histogram:\n",
      "[  39  125  828 2326  900 2255   36   47   25    0]\n",
      "Client 2 label histogram:\n",
      "[   1    0  824    0   43 1779  980  162 4544  300]\n",
      "Client 3 label histogram:\n",
      "[   0    8  382  469   59   30    3   88  677 1264]\n",
      "Client 4 label histogram:\n",
      "[359   1  12 125   3   0  42  55 255   0]\n",
      "Client 5 label histogram:\n",
      "[   9 2469  223  424 4598  727  248    0    0 2080]\n",
      "Client 6 label histogram:\n",
      "[3460    0 1190 1027    3  449 1685 1130   12  333]\n",
      "Client 7 label histogram:\n",
      "[198   0   0   1   0 179   0   2  30 277]\n",
      "Client 8 label histogram:\n",
      "[   0  476   16 1740    0    0  759  643    0    1]\n",
      "Client 9 label histogram:\n",
      "[ 456  107    1   10  125    1 2126    1    4  401]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Example usage\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_CLIENTS = 10\n",
    "    ALPHA = 5  # smaller => more non-IID        # We decided to use alpha of 5 for the experiments\n",
    "\n",
    "    trainset, client_idcs, client_loaders = get_MNIST_dirichlet_clients(\n",
    "        data_root=\"/local/scratch/a/dalwis/single_agent_RL_for_pFL/data\",\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        alpha=ALPHA,\n",
    "        batch_size=64,\n",
    "        seed=123,\n",
    "    )\n",
    "\n",
    "    # Quick sanity check: print per-client sizes\n",
    "    for cid, idxs in client_idcs.items():\n",
    "        print(f\"Client {cid}: {len(idxs)} samples\")\n",
    "\n",
    "    for i in range(NUM_CLIENTS):\n",
    "      # Optional: check label distribution for a client\n",
    "      labels = np.array(trainset.targets)\n",
    "      cid = i\n",
    "      client_labels = labels[client_idcs[cid]]\n",
    "      print(f\"Client {cid} label histogram:\")\n",
    "      print(np.bincount(client_labels, minlength=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e5fca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Client 0: 4827 samples\n",
      "Client 1: 5544 samples\n",
      "Client 2: 3847 samples\n",
      "Client 3: 4373 samples\n",
      "Client 4: 4191 samples\n",
      "Client 5: 4851 samples\n",
      "Client 6: 4670 samples\n",
      "Client 7: 5662 samples\n",
      "Client 8: 5713 samples\n",
      "Client 9: 6322 samples\n",
      "Saved Client 0: 4827 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_0\n",
      "Saved Client 1: 5544 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_1\n",
      "Saved Client 2: 3847 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_2\n",
      "Saved Client 3: 4373 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_3\n",
      "Saved Client 4: 4191 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_4\n",
      "Saved Client 5: 4851 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_5\n",
      "Saved Client 6: 4670 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_6\n",
      "Saved Client 7: 5662 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_7\n",
      "Saved Client 8: 5713 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_8\n",
      "Saved Client 9: 6322 samples → /local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1/client_9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Make CIFAR-10 Dirichlet client splits\n",
    "# ============================================================\n",
    "\n",
    "def get_cifar10_dirichlet_clients(data_root, num_clients, alpha, batch_size, seed=123):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.CIFAR10(root=data_root, train=True, download=True, transform=transform)\n",
    "\n",
    "    labels = np.array(trainset.targets)\n",
    "    num_classes = 10\n",
    "    num_samples = len(trainset)\n",
    "\n",
    "    # Group indices per class\n",
    "    class_idx = [np.where(labels == c)[0] for c in range(num_classes)]\n",
    "\n",
    "    client_idcs = {i: [] for i in range(num_clients)}\n",
    "\n",
    "    # Dirichlet sampling\n",
    "    for c in range(num_classes):\n",
    "        np.random.shuffle(class_idx[c])\n",
    "        proportions = np.random.dirichlet(alpha=[alpha] * num_clients)\n",
    "\n",
    "        # Split indices for this class according to proportions\n",
    "        proportions = (np.cumsum(proportions) * len(class_idx[c])).astype(int)[:-1]\n",
    "        split = np.split(class_idx[c], proportions)\n",
    "\n",
    "        for cid in range(num_clients):\n",
    "            client_idcs[cid].extend(split[cid])\n",
    "\n",
    "    # For reproducibility, shuffle each client's indices\n",
    "    for cid in range(num_clients):\n",
    "        np.random.shuffle(client_idcs[cid])\n",
    "\n",
    "    # Build dataloaders\n",
    "    client_loaders = {\n",
    "        cid: DataLoader(Subset(trainset, client_idcs[cid]), batch_size=batch_size, shuffle=True)\n",
    "        for cid in range(num_clients)\n",
    "    }\n",
    "\n",
    "    return trainset, client_idcs, client_loaders\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Save each client's dataset to disk\n",
    "# ============================================================\n",
    "\n",
    "def save_client_splits(save_dir, trainset, client_idcs):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    data = trainset.data        # numpy array (50000, 32, 32, 3)\n",
    "    targets = np.array(trainset.targets)\n",
    "\n",
    "    for cid, idxs in client_idcs.items():\n",
    "        cid_dir = os.path.join(save_dir, f\"client_{cid}\")\n",
    "        os.makedirs(cid_dir, exist_ok=True)\n",
    "\n",
    "        client_data = data[idxs]\n",
    "        client_targets = targets[idxs]\n",
    "\n",
    "        # Save using numpy or pickle\n",
    "        np.save(os.path.join(cid_dir, \"data.npy\"), client_data)\n",
    "        np.save(os.path.join(cid_dir, \"targets.npy\"), client_targets)\n",
    "\n",
    "        print(f\"Saved Client {cid}: {len(idxs)} samples → {cid_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Example usage\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_CLIENTS = 10\n",
    "    ALPHA = 1       # More IID (We decided to use alpha of 5 for the experiments)\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    trainset, client_idcs, client_loaders = get_cifar10_dirichlet_clients(\n",
    "        data_root=\"/local/scratch/a/dalwis/single_agent_RL_for_pFL/data\",\n",
    "        num_clients=NUM_CLIENTS,\n",
    "        alpha=1/ALPHA,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=123,\n",
    "    )\n",
    "\n",
    "    # Print sizes to confirm\n",
    "    for cid, idxs in client_idcs.items():\n",
    "        print(f\"Client {cid}: {len(idxs)} samples\")\n",
    "\n",
    "    # Save the datasets\n",
    "    save_client_splits(\n",
    "        save_dir=\"/local/scratch/a/dalwis/single_agent_RL_for_pFL/data/CIFAR10/split_10_clients_alpha_1\",\n",
    "        trainset=trainset,\n",
    "        client_idcs=client_idcs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9754ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compositor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
