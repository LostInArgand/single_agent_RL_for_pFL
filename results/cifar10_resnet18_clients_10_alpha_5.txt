=== Global Round 1/10 ===
  Client 0: depth=2, local_acc=0.3867
  Client 1: depth=3, local_acc=0.4572
  Client 2: depth=3, local_acc=0.3333
  Client 3: depth=2, local_acc=0.3470
  Client 4: depth=2, local_acc=0.5682
  Client 5: depth=4, local_acc=0.5891
  Client 6: depth=6, local_acc=0.3466
  Client 7: depth=5, local_acc=0.6602
  Client 8: depth=3, local_acc=0.8782
  Client 9: depth=3, local_acc=0.7165
  Client 0: mean_dist=3.30, reward=0.3864
  Client 1: mean_dist=3.90, reward=0.4568
  Client 2: mean_dist=3.89, reward=0.3329
  Client 3: mean_dist=3.66, reward=0.3467
  Client 4: mean_dist=4.08, reward=0.5677
  Client 5: mean_dist=4.58, reward=0.5886
  Client 6: mean_dist=3.82, reward=0.3462
  Client 7: mean_dist=5.45, reward=0.6597
  Client 8: mean_dist=4.10, reward=0.8778
  Client 9: mean_dist=4.38, reward=0.7160
  RL policy loss: -0.004421
Global model accuracy (on all client data): 0.1281

=== Global Round 2/10 ===
  Client 0: depth=4, local_acc=0.6342
  Client 1: depth=5, local_acc=0.6439
  Client 2: depth=1, local_acc=0.0635
  Client 3: depth=5, local_acc=0.8255
  Client 4: depth=5, local_acc=0.7983
  Client 5: depth=3, local_acc=0.5278
  Client 6: depth=2, local_acc=0.2467
  Client 7: depth=2, local_acc=0.5460
  Client 8: depth=1, local_acc=0.0264
  Client 9: depth=4, local_acc=0.7414
  Client 0: mean_dist=3.33, reward=0.6338
  Client 1: mean_dist=4.06, reward=0.6435
  Client 2: mean_dist=2.99, reward=0.0632
  Client 3: mean_dist=4.28, reward=0.8250
  Client 4: mean_dist=4.52, reward=0.7978
  Client 5: mean_dist=3.67, reward=0.5275
  Client 6: mean_dist=2.91, reward=0.2464
  Client 7: mean_dist=3.52, reward=0.5457
  Client 8: mean_dist=2.87, reward=0.0262
  Client 9: mean_dist=4.21, reward=0.7410
  RL policy loss: -0.000305
Global model accuracy (on all client data): 0.1908

=== Global Round 3/10 ===
  Client 0: depth=1, local_acc=0.0323
  Client 1: depth=1, local_acc=0.3252
  Client 2: depth=2, local_acc=0.3164
  Client 3: depth=3, local_acc=0.7220
  Client 4: depth=2, local_acc=0.6340
  Client 5: depth=5, local_acc=0.6169
  Client 6: depth=6, local_acc=0.5737
  Client 7: depth=2, local_acc=0.5320
  Client 8: depth=1, local_acc=0.0317
  Client 9: depth=6, local_acc=0.8570
  Client 0: mean_dist=2.55, reward=0.0320
  Client 1: mean_dist=2.53, reward=0.3250
  Client 2: mean_dist=3.10, reward=0.3161
  Client 3: mean_dist=3.65, reward=0.7216
  Client 4: mean_dist=3.39, reward=0.6337
  Client 5: mean_dist=4.37, reward=0.6164
  Client 6: mean_dist=3.15, reward=0.5734
  Client 7: mean_dist=3.31, reward=0.5317
  Client 8: mean_dist=2.69, reward=0.0315
  Client 9: mean_dist=4.52, reward=0.8565
  RL policy loss: 0.012059
Global model accuracy (on all client data): 0.2381

=== Global Round 4/10 ===
  Client 0: depth=3, local_acc=0.6171
  Client 1: depth=1, local_acc=0.4271
  Client 2: depth=1, local_acc=0.0598
  Client 3: depth=1, local_acc=0.0147
  Client 4: depth=1, local_acc=0.1021
  Client 5: depth=1, local_acc=0.4440
  Client 6: depth=5, local_acc=0.4347
  Client 7: depth=4, local_acc=0.6507
  Client 8: depth=1, local_acc=0.0425
  Client 9: depth=2, local_acc=0.8299
  Client 0: mean_dist=2.42, reward=0.6168
  Client 1: mean_dist=2.04, reward=0.4268
  Client 2: mean_dist=2.21, reward=0.0596
  Client 3: mean_dist=2.32, reward=0.0145
  Client 4: mean_dist=2.44, reward=0.1018
  Client 5: mean_dist=2.16, reward=0.4438
  Client 6: mean_dist=2.82, reward=0.4344
  Client 7: mean_dist=4.18, reward=0.6503
  Client 8: mean_dist=2.28, reward=0.0423
  Client 9: mean_dist=2.72, reward=0.8296
  RL policy loss: 0.007594
Global model accuracy (on all client data): 0.2738

=== Global Round 5/10 ===
  Client 0: depth=5, local_acc=0.7348
  Client 1: depth=3, local_acc=0.6190
  Client 2: depth=1, local_acc=0.0747
  Client 3: depth=2, local_acc=0.3397
  Client 4: depth=5, local_acc=0.8200
  Client 5: depth=2, local_acc=0.6444
  Client 6: depth=2, local_acc=0.3515
  Client 7: depth=5, local_acc=0.7355
  Client 8: depth=4, local_acc=0.9097
  Client 9: depth=6, local_acc=0.8006
  Client 0: mean_dist=3.62, reward=0.7344
  Client 1: mean_dist=3.41, reward=0.6187
  Client 2: mean_dist=3.14, reward=0.0744
  Client 3: mean_dist=3.59, reward=0.3393
  Client 4: mean_dist=4.45, reward=0.8195
  Client 5: mean_dist=3.26, reward=0.6441
  Client 6: mean_dist=3.00, reward=0.3512
  Client 7: mean_dist=4.80, reward=0.7350
  Client 8: mean_dist=4.33, reward=0.9093
  Client 9: mean_dist=4.59, reward=0.8002
  RL policy loss: -0.004551
Global model accuracy (on all client data): 0.2814

=== Global Round 6/10 ===
  Client 0: depth=5, local_acc=0.7633
  Client 1: depth=6, local_acc=0.4800
  Client 2: depth=4, local_acc=0.5489
  Client 3: depth=1, local_acc=0.1421
  Client 4: depth=5, local_acc=0.7954
  Client 5: depth=5, local_acc=0.7883
  Client 6: depth=5, local_acc=0.5458
  Client 7: depth=1, local_acc=0.5633
  Client 8: depth=3, local_acc=0.8879
  Client 9: depth=4, local_acc=0.8453
  Client 0: mean_dist=3.62, reward=0.7629
  Client 1: mean_dist=4.19, reward=0.4796
  Client 2: mean_dist=3.91, reward=0.5485
  Client 3: mean_dist=3.28, reward=0.1418
  Client 4: mean_dist=4.40, reward=0.7950
  Client 5: mean_dist=4.33, reward=0.7879
  Client 6: mean_dist=3.55, reward=0.5454
  Client 7: mean_dist=3.13, reward=0.5629
  Client 8: mean_dist=4.44, reward=0.8875
  Client 9: mean_dist=3.83, reward=0.8449
  RL policy loss: -0.010247
  Global model accuracy (on all client data): 0.2312

=== Global Round 7/10 ===
  Client 0: depth=2, local_acc=0.3816
  Client 1: depth=6, local_acc=0.6507
  Client 2: depth=3, local_acc=0.5026
  Client 3: depth=2, local_acc=0.2494
  Client 4: depth=3, local_acc=0.7991
  Client 5: depth=3, local_acc=0.7604
  Client 6: depth=4, local_acc=0.4430
  Client 7: depth=2, local_acc=0.5955
  Client 8: depth=2, local_acc=0.0746
  Client 9: depth=2, local_acc=0.7946
  Client 0: mean_dist=2.86, reward=0.3814
  Client 1: mean_dist=4.02, reward=0.6503
  Client 2: mean_dist=3.34, reward=0.5023
  Client 3: mean_dist=3.44, reward=0.2490
  Client 4: mean_dist=3.56, reward=0.7987
  Client 5: mean_dist=3.57, reward=0.7600
  Client 6: mean_dist=2.97, reward=0.4427
  Client 7: mean_dist=3.32, reward=0.5951
  Client 8: mean_dist=3.53, reward=0.0743
  Client 9: mean_dist=3.42, reward=0.7943
  RL policy loss: -0.004265
Global model accuracy (on all client data): 0.2988

=== Global Round 8/10 ===
  Client 0: depth=6, local_acc=0.7095
  Client 1: depth=2, local_acc=0.7272
  Client 2: depth=5, local_acc=0.6034
  Client 3: depth=3, local_acc=0.5859
  Client 4: depth=5, local_acc=0.6125
  Client 5: depth=5, local_acc=0.7296
  Client 6: depth=3, local_acc=0.7044
  Client 7: depth=3, local_acc=0.5401
  Client 8: depth=1, local_acc=0.0510
  Client 9: depth=3, local_acc=0.8764
  Client 0: mean_dist=3.44, reward=0.7091
  Client 1: mean_dist=3.11, reward=0.7269
  Client 2: mean_dist=4.13, reward=0.6029
  Client 3: mean_dist=4.11, reward=0.5855
  Client 4: mean_dist=4.03, reward=0.6121
  Client 5: mean_dist=4.23, reward=0.7292
  Client 6: mean_dist=3.06, reward=0.7041
  Client 7: mean_dist=3.85, reward=0.5397
  Client 8: mean_dist=3.20, reward=0.0507
  Client 9: mean_dist=3.89, reward=0.8760
  RL policy loss: 0.004417
Global model accuracy (on all client data): 0.2747

=== Global Round 9/10 ===
  Client 0: depth=1, local_acc=0.3329
  Client 1: depth=5, local_acc=0.8034
  Client 2: depth=3, local_acc=0.6340
  Client 3: depth=4, local_acc=0.8346
  Client 4: depth=6, local_acc=0.7488
  Client 5: depth=6, local_acc=0.7795
  Client 6: depth=4, local_acc=0.7114
  Client 7: depth=3, local_acc=0.7584
  Client 8: depth=2, local_acc=0.0872
  Client 9: depth=4, local_acc=0.9058
  Client 0: mean_dist=3.02, reward=0.3326
  Client 1: mean_dist=3.98, reward=0.8030
  Client 2: mean_dist=3.58, reward=0.6336
  Client 3: mean_dist=4.09, reward=0.8342
  Client 4: mean_dist=4.45, reward=0.7484
  Client 5: mean_dist=4.88, reward=0.7790
  Client 6: mean_dist=3.32, reward=0.7111
  Client 7: mean_dist=3.87, reward=0.7580
  Client 8: mean_dist=3.90, reward=0.0869
  Client 9: mean_dist=3.94, reward=0.9054
  RL policy loss: -0.013012
  Global model accuracy (on all client data): 0.2976

=== Global Round 10/10 ===
  Client 0: depth=5, local_acc=0.7563
  Client 1: depth=4, local_acc=0.7016
  Client 2: dep`th=3, local_acc=0.5711
  Client 3: depth=4, local_acc=0.7694
  Client 4: depth=4, local_acc=0.8082
  Client 5: depth=5, local_acc=0.7959
  Client 6: depth=5, local_acc=0.7435
  Client 7: depth=1, local_acc=0.6955
  Client 8: depth=6, local_acc=0.9134
  Client 9: depth=1, local_acc=0.7193
  Client 0: mean_dist=3.44, reward=0.7560
  Client 1: mean_dist=3.57, reward=0.7013
  Client 2: mean_dist=3.53, reward=0.5707
  Client 3: mean_dist=3.94, reward=0.7690
  Client 4: mean_dist=3.72, reward=0.8078
  Client 5: mean_dist=4.31, reward=0.7955
  Client 6: mean_dist=3.35, reward=0.7432
  Client 7: mean_dist=2.95, reward=0.6952
  Client 8: mean_dist=4.00, reward=0.9130
  Client 9: mean_dist=3.16, reward=0.7190
  RL policy loss: 0.007356
Global model accuracy (on all client data): 0.3780

Training finished.
Saved global model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/global_model.pt
Saved client 0 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_0_model.pt
Saved client 1 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_1_model.pt
Saved client 2 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_2_model.pt
Saved client 3 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_3_model.pt
Saved client 4 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_4_model.pt
Saved client 5 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_5_model.pt
Saved client 6 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_6_model.pt
Saved client 7 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_7_model.pt
Saved client 8 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_8_model.pt
Saved client 9 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/client_9_model.pt
Saved RL policy network → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_10_clients_resnet18/policy_net.pt