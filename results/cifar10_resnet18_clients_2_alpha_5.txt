Using device: cuda
RL will choose depth in [1, 6] for each client.

=== Global Round 1/50 ===
  Client 0: depth=2, local_acc=0.4732
  Client 1: depth=3, local_acc=0.4578
  Client 0: mean_dist=10.74, reward=0.3658
  Client 1: mean_dist=10.74, reward=0.3504
  RL policy loss: 0.001382
Global model accuracy (on all client data): 0.1478

=== Global Round 2/50 ===
  Client 0: depth=3, local_acc=0.7128
  Client 1: depth=2, local_acc=0.6353
  Client 0: mean_dist=8.84, reward=0.6244
  Client 1: mean_dist=8.84, reward=0.5470
  RL policy loss: -0.005631
Global model accuracy (on all client data): 0.1906

=== Global Round 3/50 ===
  Client 0: depth=2, local_acc=0.7030
  Client 1: depth=4, local_acc=0.7903
  Client 0: mean_dist=9.98, reward=0.6031
  Client 1: mean_dist=9.98, reward=0.6905
  RL policy loss: -0.007789
Global model accuracy (on all client data): 0.4415

=== Global Round 4/50 ===
  Client 0: depth=6, local_acc=0.8296
  Client 1: depth=6, local_acc=0.9160
  Client 0: mean_dist=12.25, reward=0.7070
  Client 1: mean_dist=12.25, reward=0.7935
  RL policy loss: -0.001505
Global model accuracy (on all client data): 0.3613

=== Global Round 5/50 ===
  Client 0: depth=3, local_acc=0.8667
  Client 1: depth=3, local_acc=0.8315
  Client 0: mean_dist=10.44, reward=0.7624
  Client 1: mean_dist=10.44, reward=0.7271
  RL policy loss: -0.000282
Global model accuracy (on all client data): 0.4938

=== Global Round 6/50 ===
  Client 0: depth=4, local_acc=0.8718
  Client 1: depth=5, local_acc=0.9306
  Client 0: mean_dist=11.49, reward=0.7568
  Client 1: mean_dist=11.49, reward=0.8156
  RL policy loss: 0.007443
Global model accuracy (on all client data): 0.6276

=== Global Round 7/50 ===
  Client 0: depth=1, local_acc=0.5438
  Client 1: depth=5, local_acc=0.9661
  Client 0: mean_dist=10.36, reward=0.4402
  Client 1: mean_dist=10.36, reward=0.8625
  RL policy loss: 0.027987
Global model accuracy (on all client data): 0.6356

=== Global Round 8/50 ===
  Client 0: depth=5, local_acc=0.9089
  Client 1: depth=3, local_acc=0.9991
  Client 0: mean_dist=9.71, reward=0.8119
  Client 1: mean_dist=9.71, reward=0.9020
  RL policy loss: -0.005625
Global model accuracy (on all client data): 0.6925

=== Global Round 9/50 ===
  Client 0: depth=2, local_acc=0.9723
  Client 1: depth=2, local_acc=0.9702
  Client 0: mean_dist=7.72, reward=0.8950
  Client 1: mean_dist=7.72, reward=0.8929
  RL policy loss: 0.000027
Global model accuracy (on all client data): 0.7764

=== Global Round 10/50 ===
  Client 0: depth=1, local_acc=0.8591
  Client 1: depth=4, local_acc=0.9976
  Client 0: mean_dist=6.71, reward=0.7919
  Client 1: mean_dist=6.71, reward=0.9305
  RL policy loss: -0.000882
Global model accuracy (on all client data): 0.7229

=== Global Round 11/50 ===
  Client 0: depth=4, local_acc=0.9965
  Client 1: depth=6, local_acc=0.9826
  Client 0: mean_dist=12.36, reward=0.8728
  Client 1: mean_dist=12.36, reward=0.8589
  RL policy loss: -0.000625
Global model accuracy (on all client data): 0.7367

=== Global Round 12/50 ===
  Client 0: depth=2, local_acc=0.8864
  Client 1: depth=6, local_acc=0.9813
  Client 0: mean_dist=11.84, reward=0.7680
  Client 1: mean_dist=11.84, reward=0.8629
  RL policy loss: -0.010725
Global model accuracy (on all client data): 0.7342

=== Global Round 13/50 ===
  Client 0: depth=2, local_acc=0.7766
  Client 1: depth=5, local_acc=0.9980
  Client 0: mean_dist=11.12, reward=0.6654
  Client 1: mean_dist=11.12, reward=0.8868
  RL policy loss: -0.008625
Global model accuracy (on all client data): 0.6347

=== Global Round 14/50 ===
  Client 0: depth=6, local_acc=0.9509
  Client 1: depth=2, local_acc=1.0000
  Client 0: mean_dist=10.34, reward=0.8475
  Client 1: mean_dist=10.34, reward=0.8965
  RL policy loss: 0.004074
Global model accuracy (on all client data): 0.7045

=== Global Round 15/50 ===
  Client 0: depth=1, local_acc=0.9508
  Client 1: depth=6, local_acc=0.9924
  Client 0: mean_dist=9.29, reward=0.8579
  Client 1: mean_dist=9.29, reward=0.8995
  RL policy loss: -0.001041
Global model accuracy (on all client data): 0.7244

=== Global Round 16/50 ===
  Client 0: depth=3, local_acc=0.9975
  Client 1: depth=1, local_acc=0.9988
  Client 0: mean_dist=8.02, reward=0.9173
  Client 1: mean_dist=8.02, reward=0.9186
  RL policy loss: -0.000025
Global model accuracy (on all client data): 0.8040

=== Global Round 17/50 ===
  Client 0: depth=1, local_acc=0.8485
  Client 1: depth=1, local_acc=0.9494
  Client 0: mean_dist=6.45, reward=0.7840
  Client 1: mean_dist=6.45, reward=0.8848
  RL policy loss: -0.004083
Global model accuracy (on all client data): 0.8238

=== Global Round 18/50 ===
  Client 0: depth=4, local_acc=0.9993
  Client 1: depth=1, local_acc=0.9537
  Client 0: mean_dist=7.25, reward=0.9268
  Client 1: mean_dist=7.25, reward=0.8812
  RL policy loss: -0.001609
Global model accuracy (on all client data): 0.8003

=== Global Round 19/50 ===
  Client 0: depth=5, local_acc=0.9867
  Client 1: depth=4, local_acc=1.0000
  Client 0: mean_dist=10.51, reward=0.8817
  Client 1: mean_dist=10.51, reward=0.8949
  RL policy loss: -0.000993
Global model accuracy (on all client data): 0.8863

=== Global Round 20/50 ===
  Client 0: depth=1, local_acc=0.9519
  Client 1: depth=2, local_acc=1.0000
  Client 0: mean_dist=4.71, reward=0.9048
  Client 1: mean_dist=4.71, reward=0.9529
  RL policy loss: 0.003761
Global model accuracy (on all client data): 0.8862

=== Global Round 21/50 ===
  Client 0: depth=5, local_acc=0.9933
  Client 1: depth=3, local_acc=1.0000
  Client 0: mean_dist=7.57, reward=0.9176
  Client 1: mean_dist=7.57, reward=0.9243
  RL policy loss: -0.000411
Global model accuracy (on all client data): 0.8280

=== Global Round 22/50 ===
  Client 0: depth=1, local_acc=0.9933
  Client 1: depth=2, local_acc=0.9998
  Client 0: mean_dist=5.40, reward=0.9393
  Client 1: mean_dist=5.40, reward=0.9458
  RL policy loss: 0.000498
Global model accuracy (on all client data): 0.8938

=== Global Round 23/50 ===
  Client 0: depth=5, local_acc=0.9984
  Client 1: depth=2, local_acc=0.9999
  Client 0: mean_dist=8.08, reward=0.9176
  Client 1: mean_dist=8.08, reward=0.9191
  RL policy loss: 0.000039
Global model accuracy (on all client data): 0.7003

=== Global Round 24/50 ===
  Client 0: depth=4, local_acc=0.9999
  Client 1: depth=5, local_acc=0.9999
  Client 0: mean_dist=8.50, reward=0.9149
  Client 1: mean_dist=8.50, reward=0.9148
  RL policy loss: -0.000007
Global model accuracy (on all client data): 0.8808

=== Global Round 25/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=6, local_acc=1.0000
  Client 0: mean_dist=4.92, reward=0.9508
  Client 1: mean_dist=4.92, reward=0.9508
  RL policy loss: -0.000002
Global model accuracy (on all client data): 0.9181

=== Global Round 26/50 ===
  Client 0: depth=5, local_acc=0.9965
  Client 1: depth=6, local_acc=0.9997
  Client 0: mean_dist=3.46, reward=0.9619
  Client 1: mean_dist=3.46, reward=0.9651
  RL policy loss: -0.000271
Global model accuracy (on all client data): 0.8988

=== Global Round 27/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=1, local_acc=0.9952
  Client 0: mean_dist=3.55, reward=0.9645
  Client 1: mean_dist=3.55, reward=0.9596
  RL policy loss: -0.000213
Global model accuracy (on all client data): 0.8759

=== Global Round 28/50 ===
  Client 0: depth=5, local_acc=0.9996
  Client 1: depth=3, local_acc=1.0000
  Client 0: mean_dist=5.98, reward=0.9398
  Client 1: mean_dist=5.98, reward=0.9402
  RL policy loss: -0.000027
Global model accuracy (on all client data): 0.9166

=== Global Round 29/50 ===
  Client 0: depth=5, local_acc=1.0000
  Client 1: depth=1, local_acc=0.9682
  Client 0: mean_dist=3.74, reward=0.9626
  Client 1: mean_dist=3.74, reward=0.9308
  RL policy loss: 0.002898
Global model accuracy (on all client data): 0.8093

=== Global Round 30/50 ===
  Client 0: depth=3, local_acc=1.0000
  Client 1: depth=6, local_acc=0.9982
  Client 0: mean_dist=8.68, reward=0.9132
  Client 1: mean_dist=8.68, reward=0.9114
  RL policy loss: 0.000004
Global model accuracy (on all client data): 0.8269

=== Global Round 31/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=6, local_acc=0.9994
  Client 0: mean_dist=8.37, reward=0.9163
  Client 1: mean_dist=8.37, reward=0.9157
  RL policy loss: -0.000030
Global model accuracy (on all client data): 0.8769

=== Global Round 32/50 ===
  Client 0: depth=3, local_acc=1.0000
  Client 1: depth=2, local_acc=1.0000
  Client 0: mean_dist=4.38, reward=0.9562
  Client 1: mean_dist=4.38, reward=0.9562
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9218

=== Global Round 33/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=3, local_acc=1.0000
  Client 0: mean_dist=3.20, reward=0.9680
  Client 1: mean_dist=3.20, reward=0.9680
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9231

=== Global Round 34/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=6, local_acc=1.0000
  Client 0: mean_dist=4.30, reward=0.9570
  Client 1: mean_dist=4.30, reward=0.9570
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9238

=== Global Round 35/50 ===
  Client 0: depth=2, local_acc=1.0000
  Client 1: depth=2, local_acc=1.0000
  Client 0: mean_dist=3.49, reward=0.9651
  Client 1: mean_dist=3.49, reward=0.9651
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9369

=== Global Round 36/50 ===
  Client 0: depth=6, local_acc=0.9958
  Client 1: depth=2, local_acc=1.0000
  Client 0: mean_dist=8.41, reward=0.9117
  Client 1: mean_dist=8.41, reward=0.9159
  RL policy loss: 0.000374
Global model accuracy (on all client data): 0.8484

=== Global Round 37/50 ===
  Client 0: depth=5, local_acc=1.0000
  Client 1: depth=3, local_acc=0.9997
  Client 0: mean_dist=5.34, reward=0.9466
  Client 1: mean_dist=5.34, reward=0.9464
  RL policy loss: 0.000018
Global model accuracy (on all client data): 0.9310

=== Global Round 38/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=5, local_acc=1.0000
  Client 0: mean_dist=3.93, reward=0.9607
  Client 1: mean_dist=3.93, reward=0.9607
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9498

=== Global Round 39/50 ===
  Client 0: depth=3, local_acc=1.0000
  Client 1: depth=4, local_acc=1.0000
  Client 0: mean_dist=2.62, reward=0.9738
  Client 1: mean_dist=2.62, reward=0.9738
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9578

=== Global Round 40/50 ===
  Client 0: depth=1, local_acc=0.9985
  Client 1: depth=3, local_acc=1.0000
  Client 0: mean_dist=5.11, reward=0.9474
  Client 1: mean_dist=5.11, reward=0.9489
  RL policy loss: -0.000028
Global model accuracy (on all client data): 0.8911

=== Global Round 41/50 ===
  Client 0: depth=1, local_acc=0.9639
  Client 1: depth=5, local_acc=1.0000
  Client 0: mean_dist=4.15, reward=0.9224
  Client 1: mean_dist=4.15, reward=0.9585
  RL policy loss: 0.000972
Global model accuracy (on all client data): 0.8641

=== Global Round 42/50 ===
  Client 0: depth=3, local_acc=1.0000
  Client 1: depth=4, local_acc=0.9998
  Client 0: mean_dist=6.20, reward=0.9380
  Client 1: mean_dist=6.20, reward=0.9378
  RL policy loss: -0.000001
Global model accuracy (on all client data): 0.9499

=== Global Round 43/50 ===
  Client 0: depth=6, local_acc=0.9943
  Client 1: depth=6, local_acc=1.0000
  Client 0: mean_dist=7.55, reward=0.9189
  Client 1: mean_dist=7.55, reward=0.9245
  RL policy loss: -0.000130
Global model accuracy (on all client data): 0.9338

=== Global Round 44/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=3, local_acc=1.0000
  Client 0: mean_dist=2.56, reward=0.9744
  Client 1: mean_dist=2.56, reward=0.9744
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9620

=== Global Round 45/50 ===
  Client 0: depth=2, local_acc=1.0000
  Client 1: depth=4, local_acc=1.0000
  Client 0: mean_dist=2.67, reward=0.9733
  Client 1: mean_dist=2.67, reward=0.9733
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9466

=== Global Round 46/50 ===
  Client 0: depth=5, local_acc=1.0000
  Client 1: depth=4, local_acc=1.0000
  Client 0: mean_dist=2.50, reward=0.9750
  Client 1: mean_dist=2.50, reward=0.9750
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9634

=== Global Round 47/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=4, local_acc=1.0000
  Client 0: mean_dist=3.68, reward=0.9632
  Client 1: mean_dist=3.68, reward=0.9632
  RL policy loss: 0.000000
Global model accuracy (on all client data): 0.9580

=== Global Round 48/50 ===
  Client 0: depth=4, local_acc=1.0000
  Client 1: depth=5, local_acc=0.9995
  Client 0: mean_dist=4.86, reward=0.9514
  Client 1: mean_dist=4.86, reward=0.9509
  RL policy loss: -0.000058
Global model accuracy (on all client data): 0.9478

=== Global Round 49/50 ===
  Client 0: depth=5, local_acc=0.9992
  Client 1: depth=1, local_acc=0.9999
  Client 0: mean_dist=5.55, reward=0.9437
  Client 1: mean_dist=5.55, reward=0.9445
  RL policy loss: -0.000065
Global model accuracy (on all client data): 0.9067

=== Global Round 50/50 ===
  Client 0: depth=6, local_acc=1.0000
  Client 1: depth=1, local_acc=0.9859
  Client 0: mean_dist=3.48, reward=0.9652
  Client 1: mean_dist=3.48, reward=0.9511
  RL policy loss: 0.000351
Global model accuracy (on all client data): 0.8720

Training finished.
Saved global model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_2_clients_resnet18/global_model.pt
Saved client 0 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_2_clients_resnet18/client_0_model.pt
Saved client 1 model → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_2_clients_resnet18/client_1_model.pt
Saved RL policy network → /local/scratch/a/dalwis/single_agent_RL_for_pFL/src/weights/split_2_clients_resnet18/policy_net.pt
